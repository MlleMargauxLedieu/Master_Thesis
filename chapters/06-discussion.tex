\chapter{Discussion}\label{chp:discussion}
\section{Hyper parameters influence}
\subsection{Initialization}
For every network and every class, we tried two different intiializations. For the first two networks: AlexNet and Resnet18, the pretrained weights achieve better results. For the inception network, it is the opposite, the random initialization performs better. 
We can also not that when training Resnet and AlexNet, we could use 700 samples per batch to run two experience in parallel. While for Inception, we could only have a batch size of 50 to run the experiment. This is likely because the inception network is deeper so it takes more memory to train.

\subsubsection{Porosity}
For AlexNet and ResNet18, the porosity class performs particularly bass. It starts with around the same accuracy as pretrained, but then it decreases over the epochs. We have only four labels in this class, and at the first iteration, the random initialization only predicts 0 as label. 
For example when the validation set has label as follows : 1086 samples have no\_vp, 234 have vp\_minor, 255 have vp\_intermediate and 104 have vp\_abundant. We have a total of 1679 samples in our validation set. If the network predicts only 0 after the first epoch, then its accuracy will be \(1086/1679 = 0,65\) 65\%. So we have to be careful with the actual value that the network is predicting.  High accuracy on the test set does no necessarily means that the model is actually performing well. That is why we use confusion matrix to see how the labels are predicted.

If we look into the details of the plots, we can see that almost all training starts with an accuracy around 65\%. Then as the network learns, it will start predicting other labels and that can reduce its performances. This is what happens for the random initialization for AlexNet and ResNet18. the final accuracy is not as good as the initial one. We can conclude that the network does not learn when it uses random initialization for the porosity labels.

For the Inception network, both initial accuracy are around 65\% again. During the training with pretrained weights we have a constant small increase. At then end of the training with pretrained weights we have a 8\% improvement in accuracy. While with random initialization, the increase is more fluctuating and bigger. After training, the network has improved its accuracy by 30\% at best.



\subsubsection{Dunham, DRT and Components}
All three classes behave similarly on the different networks. Once again, for AlexNet and ResNet18, pretrained weights achieve a more stable and better learning. We have more labels in those two classes, respectively 6, 13 and 19 for Dunham, Components and DRT. 
For Incepetion-v3, the initial accuracy is better with the pretrained weights. But after a few epochs, the randomly initialized network catches up. In the end, it is random initialization which achieves best results again. We note that there is a very high fluctuations. This is most likely because of the optimizer.

The fact that we have such big fluctuations in the training with random initialization is our choice of optimizer. Indeed, for Inception\_v3, we only train for 30 epochs because the training is very long. So we are in the early phase of training, meaning we have a big learning rate. Also, as Inception\_v3 is a bigger network, we see  that the small amount of data is not enough for high learning rate to make it converge. The solution would be to try for more epochs to see how it stabilizes near the end of the training. 

\subsubsection{Conclusion}
From this experiment, we can conclude that for AlexNet and ResNet18, the pretrained weights is the best initialization. For Inception-v3, random initialization achieves best results. We present the best results achieved by each network on each class in table \ref{tab:finalinit}
\begin{table}
\caption{\label{tab:googinit} The results of training Inception network using random initialization or pretrained weights with Adamax as optimizer}
\centering
\begin{tabular}[b]{| l | l | l | l | l |}
\hline
    Initialization & Class & Validation accuracy  & Test accuracy\\ \hline
    \multirow{4}{*}{Resnet 18} & Dunham &  61,9\%  & 37,3\% \\ %%cf Confusion Matrix 
    & Porosity & 74,1\% &  57,8& \\
    &DRT & 55,0\% &  25,3\% \\
    &Components & 69,8\% &  44,7\% \\ \hline
     \multirow{4}{*}{AlexNet} & Dunham &  81,8\% & 36,8\% \\
    & Porosity & 89,2\% &  68,7\% \\
    &DRT & 74,8\% &   23,6\% \\
    &Components & 85,0\% & 50,2\% \\ \hline
    \multirow{4}{*}{Inception v3} & Dunham &  90,1\% & 66,2\% \\
    & Porosity & 97,5\% &  76,1\% \\
    &DRT & 99,2\% &  53,7\% \\
    &Components & 95,6\% & 44,6\% \\ \hline
\end{tabular} 
\end{table}
Overall, the best results were achieved with Inception\_v3, this is why we chose to try to optimize it further using a different optimizer combined with a longer training time. 

\subsection{Optimizers}
As we mentionned in the previouss ection, the network that gave the overall best results is Inception\_v3.
This is why we chose it to optimize further. 
\section{Classification Task}
\subsection{Resnet18}
For Resnet18, the best validation accuracy we achieved was 74\% on the porosity class, and 58\% for the test set. The confusion matrix on Figure \ref{fig:rescm_poro} shows that it confuses no\_vp and vp\_minor. But it is good at predicting vp\_abundant. Something that is consistent for all 4 classes is that Resnet will perform best on the labels that is most populated. That is logical because it is the one that the networks has seen most of so it knows best how to identify it. Resnet18 is overfitting, the results on the test set are very inferior to the ones on the validation test. A slightly worse results is acceptable, but the ones we get from this network are not acceptable. This can not be used to help geologists since we can not trust the results well enough.

\subsection{AlexNet}
AlexNet outperforms Resnet 18. It gives better results in validation accuracy, but still fails to perform well on new images. Even though we get relatively high values of accuracy on the test set, there is still no clear diagonal on the confusion matrix. For Dunham, the class with the highest missclassification is 13PS-GS. This is a class that mixed both PS and GS. That is why it is difficult to predict clearly. 

\subsubsection{Inception\_v3}
The Inception network outperforms both networks for the classification task. It has both very good results on the validation dataset, but also gives correct results on new images. For porosity, we have a test accuracy up to 76\% which is really high. We can clearly see 3 squares around the diagonal. There is some misclassification between similar classes, but not with different classes. 
For Dunham, there is still confusion between the classes. We notice the same one between 12GS, 13PS-GS and 14PS. As we explained before, this one is acceptable since the middle one is a class created when one could not say if it was actually Packstone or Graystone. But the results are undeniably better. 
The  for the DRT, we can also see a diagonal. We notice a confusion between DRT09 and DRT06 which are both PS-GS with different minerals. This mistake is acceptable as well.
Finally for components, the validation results are really good. But the test results are quite disappointing. The precision is acceptable: 60\%, but the recall is only 40\%. This means that our classification is able to differentiate between different components but it misses many.

\section{Performance}
Now that we presented and discussed the theoretical aspect of our results, we can take a step back and think about what those accuracy actually means. It is known that there is a lot of disagreement between geologist, especially about carbonate rocks. If you ask four different geologists to label the same picture, they would most likely not all agree on the answer. 
Even to assess the porosity, two geologists might disagree on if it there is no porosity at all, pr a small amount. This explains why our model struggles to identify this case. But it gets even more difficult when we have to identify the depositional rock types and components. For DRT, we have to identify both the matrix of the rock but also which components are there and in which quantity. So there are multiple sources of disagreement for geologists. 

It is important to remember that all the labels have been given by the same geologist. Our prediction, will always be biased towards his or her opinion. It is this geologists knowledge that translates from this classification task. In real life, geologists might not agree on a label, but it is still valid to have someone else's opinion. This is how this prediction can be used in real uses cases. A geologist on an oil rig, labeling those thin sections, could use a prediction as a tool to make its final decision. For easy cases, it will be only validation of the prediction from the geologist. And for more complex cases, this could be considered as a colleague's opinion, that you can either agree on, or challenge if you think he is wrong. 

Now, if we consider our 76\% accuracy. If we refer to \cite{thesis_imperial}, the consensus between geologists for carbonate rocks is around XXX\%. So having a model that gives you almost as good accuracy as a corpus of geologists would is very satisfying. This puts also in perspective the results we get for different classes. It is normal that we get lower accuracy for components since there are many different sources of possible disagreements. So our accuracy might even be higher than the ones several geologists would agree to. 